{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word-embeddings.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPCKWR825UUIQpgFVgpmk5+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apresland/tensorflow-nlp/blob/word-embeddings/word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZr7vIv-qRR_"
      },
      "source": [
        "# Word Embeddings\n",
        "This notebook denonstrates creating word embeddings in tensorflow. We will train a word embeddings using a simple Keras model for a sentiment classification task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI0VdVXvpNcR"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDz_2RJsq_lE"
      },
      "source": [
        "# Download the IMDBÂ dataset\n",
        "The IMDB dataset is available as a  TensorFlow datasets. The following code downloads the IMDB dataset: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl-tKTysq_4k"
      },
      "source": [
        "# Split the training set 60%:40%, so we'll end up with 15,000 examples\n",
        "# for training, 10,000 examples for validation and 25,000 examples for testing.\n",
        "raw_train_ds, raw_val_ds, raw_test_ds = tfds.load(\n",
        "    name=\"imdb_reviews\", \n",
        "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
        "    as_supervised=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0PhTurAubAD"
      },
      "source": [
        "# Text pre-processing\n",
        "Define the dataset preprocessing steps required for the classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2bd7UETubjP"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "\n",
        "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# Normalize, split, and map strings to integers.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHPot2wi3K8N"
      },
      "source": [
        "# Create a classifiction model\n",
        "Use Keras to define the sentiment classification model in a \"continuous bag of words\" style."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FhSARe23Lan"
      },
      "source": [
        "EMBEDDING_DIM=16\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  vectorize_layer,\n",
        "  tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, name=\"embedding\"),\n",
        "  tf.keras.layers.GlobalAveragePooling1D(),\n",
        "  tf.keras.layers.Dense(16, activation='relu'),\n",
        "  tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}